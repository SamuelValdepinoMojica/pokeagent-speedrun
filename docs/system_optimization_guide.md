# ğŸ’» GuÃ­a de ConfiguraciÃ³n para Tu Sistema

## ğŸ–¥ï¸ Especificaciones de Tu Sistema

```
CPU: Intel Ultra 9 (Laptop)
GPU: RTX 5060 Ti 16GB (via oculink)
RAM: 32GB
```

---

## âš¡ RecomendaciÃ³n de Ambientes

### CÃ¡lculo de Capacidad:

**RAM disponible:** 32GB
- Sistema operativo: ~4GB
- Por ambiente: ~1.5GB (emulator + estado)
- **MÃ¡ximo teÃ³rico:** (32 - 4) / 1.5 = **~18 ambientes**

**CPU (Intel Ultra 9):**
- Cores tÃ­picos: 14-16 cores (6P + 8E)
- **Recomendado:** `n_envs = cores - 2` = **12-14 ambientes**

**GPU (RTX 5060 Ti 16GB):**
- Excelente para entrenamiento DRL
- 16GB VRAM es mÃ¡s que suficiente
- Con CNN policy, usar ~2GB VRAM

### ğŸ¯ **ConfiguraciÃ³n Ã“ptima para Tu Sistema:**

```bash
python train_ppo.py \
    --mode train \
    --timesteps 1000000 \
    --n-envs 12 \
    --frame-skip 6 \
    --state Emerald-GBAdvance/quick_start_save.state
```

**Por quÃ© 12 ambientes:**
- âœ… Aprovecha todos los cores CPU
- âœ… No satura la RAM (12 Ã— 1.5GB = 18GB)
- âœ… Deja margen para el sistema
- âœ… ~12x speedup vs 1 ambiente

---

## ğŸ“Š Rendimiento Esperado con Tu Sistema

| n_envs | RAM Usada | CPU Load | Tiempo 1M steps | Speedup |
|--------|-----------|----------|-----------------|---------|
| 1      | ~5GB      | ~10%     | ~7 horas        | 1x      |
| 4      | ~10GB     | ~40%     | ~1.8 horas      | 4x      |
| 8      | ~16GB     | ~70%     | ~55 minutos     | 8x      |
| **12** | **~22GB** | **~90%** | **~35 minutos** | **12x** |
| 16     | ~28GB     | ~100%    | ~26 minutos     | 16x âš ï¸  |

**RecomendaciÃ³n:** **12 ambientes** es el sweet spot

---

## ğŸ® 1. Ver Entrenamiento en Tiempo Real

### OpciÃ³n A: TensorBoard (Recomendado)

**Durante el entrenamiento**, en otra terminal:

```bash
python monitor_training.py
```

O manualmente:
```bash
tensorboard --logdir=./tensorboard_logs --port=6006
```

Luego abre: **http://localhost:6006**

**VerÃ¡s:**
- ğŸ“ˆ **Reward por episodio** (Â¿estÃ¡ subiendo?)
- ğŸ“ **Longitud de episodios** (Â¿explora mÃ¡s?)
- ğŸ“‰ **Policy loss** (Â¿estÃ¡ aprendiendo?)
- ğŸ² **Entropy** (Â¿estÃ¡ explorando?)

### OpciÃ³n B: Ver Logs en Terminal

Los logs muestran progress cada Ã©poca:

```
| rollout/              |          |
|    ep_len_mean        | 245      |  â† Promedio de pasos por episodio
|    ep_rew_mean        | -12.5    |  â† Reward promedio (queremos que suba)
| time/                 |          |
|    fps                | 156      |  â† Steps por segundo
|    total_timesteps    | 8192     |  â† Progreso total
```

### OpciÃ³n C: Ver al Agente Jugando (Pausar Training)

**IMPORTANTE:** No se puede visualizar DURANTE el training con mÃºltiples ambientes.

**Workflow recomendado:**

1. **Entrenar sin visualizaciÃ³n:**
```bash
python train_ppo.py --mode train --timesteps 100000 --n-envs 12
```

2. **Detener training** (Ctrl+C despuÃ©s de X steps)

3. **Ver el modelo entrenado:**
```bash
# Ver el Ãºltimo checkpoint
python watch_trained_agent.py --model logs/checkpoints/ppo_pokemon_10000_steps.zip
```

---

## ğŸ“Š 2. CÃ³mo Saber Si EstÃ¡ Aprendiendo

### âœ… SeÃ±ales de Buen Aprendizaje:

#### A. Reward Aumenta
```
Epoch 1:  ep_rew_mean = -50.0   âŒ Malo (penalizaciones)
Epoch 10: ep_rew_mean = -20.0   âš ï¸  Mejorando
Epoch 20: ep_rew_mean = 5.0     âœ… Bien!
Epoch 50: ep_rew_mean = 50.0    ğŸ‰ Excelente!
```

**Revisar en TensorBoard:** Curva de "rollout/ep_rew_mean" debe **subir**

#### B. Longitud de Episodios Aumenta
```
Epoch 1:  ep_len_mean = 50      âŒ Muere rÃ¡pido
Epoch 10: ep_len_mean = 200     âš ï¸  Sobrevive mÃ¡s
Epoch 50: ep_len_mean = 1000    âœ… Explora mucho
```

**Significa:** El agente sobrevive mÃ¡s tiempo sin morir

#### C. Entropy Disminuye Gradualmente
```
Epoch 1:  entropy = 2.0         âœ… Explora mucho (bueno al inicio)
Epoch 50: entropy = 0.5         âœ… MÃ¡s decidido (bueno al final)
```

**Significa:** El agente pasa de explorar aleatoriamente a tomar decisiones mÃ¡s seguras

#### D. Policy Loss Disminuye
```
Epoch 1:  policy_loss = 0.5     âš ï¸  Alto
Epoch 50: policy_loss = 0.05    âœ… Bajo (aprendiÃ³)
```

### âŒ SeÃ±ales de Mal Aprendizaje:

1. **Reward no aumenta despuÃ©s de 50k steps**
   - Posible problema: Reward shaping malo
   - SoluciÃ³n: Ajustar recompensas

2. **Entropy = 0 muy rÃ¡pido**
   - Problema: ColapsÃ³ a una polÃ­tica (ej: solo presiona UP)
   - SoluciÃ³n: Aumentar `ent_coef` en PPO

3. **Episode length = max_steps siempre**
   - Problema: Nunca termina (stuck en loops)
   - SoluciÃ³n: Aumentar penalizaciÃ³n por inmovilidad

---

## ğŸ 3. Sistema de Recompensas Actual

Revisa el archivo `agent/drl_env.py`:

```python
def _calculate_reward_from_lightweight(prev_state, current_state):
    reward = 0.0
    
    # ğŸ† Objetivo principal: Obtener badges
    if curr_badges > prev_badges:
        reward += 1000.0  # Â¡GRANDE! Es el objetivo
    
    # ğŸ“ˆ Subir de nivel
    if curr_levels > prev_levels:
        reward += 50.0
    
    # ğŸš¶ Moverse (explorar)
    if curr_coords != prev_coords:
        reward += 0.5  # PequeÃ±o reward por movimiento
    else:
        self.stationary_steps += 1
        reward -= 0.05 * min(self.stationary_steps, 20)  # Penaliza quedarse quieto
    
    # ğŸ’” HP bajo (penalizaciÃ³n)
    if hp_ratio < 0.2:
        reward -= 5.0  # CrÃ­tico
    elif hp_ratio < 0.5:
        reward -= 1.0  # Bajo
```

### ğŸ“Š Escala de Recompensas:

```
+1000.0  â†’ Badge obtenida (OBJETIVO PRINCIPAL)
+50.0    â†’ Subir nivel
+20.0    â†’ Descubrir nueva ubicaciÃ³n (primera vez)
+5.0     â†’ Revisitar ubicaciÃ³n
+0.5     â†’ Moverse (cada step)
-0.05    â†’ Quedarse quieto (por step)
-1.0     â†’ HP < 50%
-5.0     â†’ HP < 20%
```

### ğŸ¯ Recompensas Esperadas:

**Agente random (malo):**
```
Episode reward: -50 a -20
(Se queda quieto mucho, pierde HP)
```

**Agente explorando (mejorando):**
```
Episode reward: -10 a +20
(Se mueve, descubre lugares)
```

**Agente entrenado (bueno):**
```
Episode reward: +50 a +200
(Explora eficientemente, sube niveles)
```

**Agente experto (objetivo):**
```
Episode reward: +1000+
(Obtiene badges!)
```

---

## ğŸ§ª 4. Script de Testing

Prueba tu configuraciÃ³n:

```bash
# Test 1: Verificar que soporta 12 ambientes (30 segundos)
python train_ppo.py --mode train --timesteps 1000 --n-envs 12

# Si funciona â†’ âœ… Tu sistema soporta 12 ambientes

# Test 2: Training corto para ver rewards (5 minutos)
python train_ppo.py --mode train --timesteps 10000 --n-envs 12

# Revisar en TensorBoard si reward sube

# Test 3: Ver agente random (baseline)
python watch_trained_agent.py --random --steps 1000

# Nota el reward promedio (~-20 a -50 tÃ­picamente)

# Test 4: Training real (30 minutos)
python train_ppo.py --mode train --timesteps 100000 --n-envs 12

# DespuÃ©s, ver el agente entrenado:
python watch_trained_agent.py --model logs/checkpoints/ppo_pokemon_100000_steps.zip
```

---

## ğŸ“ˆ 5. Monitoreo Recomendado

### Setup de 2 Terminales:

**Terminal 1 - Training:**
```bash
python train_ppo.py --mode train --timesteps 1000000 --n-envs 12 --state Emerald-GBAdvance/quick_start_save.state
```

**Terminal 2 - Monitoring:**
```bash
python monitor_training.py
# O manualmente:
tensorboard --logdir=./tensorboard_logs
```

### QuÃ© Revisar Cada 10-20 Minutos:

1. **TensorBoard (navegador):**
   - Â¿Reward subiendo? âœ…
   - Â¿Episode length aumentando? âœ…
   - Â¿Policy loss bajando? âœ…

2. **Terminal del training:**
   - Â¿FPS estable? (deberÃ­a ser ~150-200 con n_envs=12)
   - Â¿Sin crashes? âœ…

3. **Recursos del sistema:**
```bash
# En otra terminal:
htop  # Ver CPU y RAM

# O:
watch -n 1 'ps aux | grep train_ppo | head -5'
```

---

## ğŸ¬ 6. Workflow Completo Recomendado

```bash
# DÃ­a 1: Training inicial (30 min - 1 hora)
python train_ppo.py --mode train --timesteps 100000 --n-envs 12
# â†’ Produce: logs/checkpoints/ppo_pokemon_100000_steps.zip

# Ver resultado:
python watch_trained_agent.py --model logs/checkpoints/ppo_pokemon_100000_steps.zip

# DÃ­a 2: Training largo (overnight)
python train_ppo.py --mode train --timesteps 1000000 --n-envs 12
# â†’ ~35 minutos con tu sistema

# DÃ­a 3: Fine-tuning
# Cargar modelo anterior y seguir entrenando
python train_ppo.py --mode train --timesteps 2000000 --n-envs 12 --model logs/checkpoints/ppo_pokemon_1000000_steps.zip
```

---

## ğŸ’¡ Tips Finales

### Para Maximizar Tu Hardware:

1. **Usa los 12 ambientes** - aprovecha tu CPU
2. **GPU se usa automÃ¡ticamente** - PyTorch detecta la RTX 5060 Ti
3. **Monitorea con TensorBoard** - visual y claro
4. **Checkpoints cada 10k steps** - no pierdas progreso

### Si el Training es Muy Lento:

1. Verifica GPU:
```python
import torch
print(torch.cuda.is_available())  # Debe ser True
print(torch.cuda.get_device_name(0))  # Debe mostrar RTX 5060 Ti
```

2. Reduce calidad si necesario:
```bash
python train_ppo.py --mode train --timesteps 1000000 --n-envs 8 --frame-skip 12
```

### Si Quieres Ver Jugando Durante Training:

**OpciÃ³n:** Entrenar 1 ambiente con visualizaciÃ³n (LENTO pero educativo)
```bash
python train_ppo.py --mode train --timesteps 10000 --n-envs 1 --visualize
```

âš ï¸ **NOTA:** Esto es ~12x mÃ¡s lento, solo para debugging/demo

---

## ğŸš€ Comando Final Recomendado Para Ti:

```bash
# Training Ã³ptimo para tu sistema (Intel Ultra 9 + 32GB RAM + RTX 5060 Ti)
python train_ppo.py \
    --mode train \
    --timesteps 1000000 \
    --n-envs 12 \
    --frame-skip 6 \
    --state Emerald-GBAdvance/quick_start_save.state

# En otra terminal (monitoreo):
python monitor_training.py
```

**Resultado esperado:**
- â±ï¸ **35 minutos** para 1M steps
- ğŸ’¾ **22GB RAM** usados
- ğŸ–¥ï¸ **~90% CPU** utilizado
- ğŸ® **~150 FPS** de training
- ğŸ’° **Checkpoints cada 10k steps**

---

**Â¿Listo para empezar el training optimizado?** ğŸƒâ€â™‚ï¸
