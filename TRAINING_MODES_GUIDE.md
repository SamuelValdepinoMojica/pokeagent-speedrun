# ğŸ® GuÃ­a de ComparaciÃ³n: 3 Modos de Entrenamiento

## ğŸ“‹ Resumen

Ahora puedes entrenar **3 modelos diferentes** para compararlos:

| Modo | DescripciÃ³n | Observation | Reward Shaping |
|------|-------------|-------------|----------------|
| ğŸ”µ **Pure DRL** | DRL puro sin ayuda | Vector [19] con milestone_count | âŒ Ninguno |
| ğŸ“Š **Rule-based** | Milestones con reglas | Vector [19] con milestone_count | âœ… Reglas fijas |
| ğŸ¤– **LLM+Dialogue** | LLM lee diÃ¡logos | Vector [19] con milestone_count | âœ… LLM inteligente |

---

## ğŸš€ Uso RÃ¡pido

### **OpciÃ³n 1: Script AutomÃ¡tico**

```bash
# Entrena los 3 modelos en secuencia
./train_comparison.sh
```

El script te preguntarÃ¡ cuÃ¡les quieres entrenar.

---

### **OpciÃ³n 2: Manual (un modelo a la vez)**

#### **1ï¸âƒ£ Pure DRL (sin reward shaping)**

```bash
python train_ppo.py \
    --mode train \
    --state Emerald-GBAdvance/quick_start_save.state \
    --timesteps 100000 \
    --n-envs 4 \
    --model-path ./models/ppo_pure_drl \
    --pure-drl
```

**CaracterÃ­sticas**:
- âœ… Milestone count en observation (vector[16])
- âŒ Sin callbacks de reward shaping
- âš¡ MÃ¡s rÃ¡pido (sin LLM overhead)

---

#### **2ï¸âƒ£ Rule-based Milestones**

```bash
python train_ppo.py \
    --mode train \
    --state Emerald-GBAdvance/quick_start_save.state \
    --timesteps 100000 \
    --n-envs 4 \
    --model-path ./models/ppo_rule_based
```

**CaracterÃ­sticas**:
- âœ… Milestone count en observation
- âœ… LLM Callback con reglas fijas:
  - `stationary > 100` â†’ 0.3Ã— penalty
  - `new milestone` â†’ 1.8Ã— boost
- âš¡ RÃ¡pido (no usa Ollama)

---

#### **3ï¸âƒ£ LLM + DiÃ¡logos**

```bash
# Primero iniciar Ollama
ollama serve

# En otra terminal:
python train_ppo.py \
    --mode train \
    --state Emerald-GBAdvance/quick_start_save.state \
    --timesteps 100000 \
    --n-envs 4 \
    --model-path ./models/ppo_llm_dialogue \
    --use-llm
```

**CaracterÃ­sticas**:
- âœ… Milestone count en observation
- âœ… LLM lee diÃ¡logos con OCR
- âœ… Decisiones inteligentes basadas en texto del juego
- ğŸŒ MÃ¡s lento (llamadas a LLM cada 1000 steps)

---

## ğŸ“Š Comparar Resultados

### **Con TensorBoard**

```bash
# Ver todos los entrenamientos juntos
tensorboard --logdir ./tensorboard_logs

# Solo ver modelos especÃ­ficos
tensorboard --logdir_spec \
    pure:./tensorboard_logs/PPO_pure_drl,\
    rules:./tensorboard_logs/PPO_rule_based,\
    llm:./tensorboard_logs/PPO_llm_dialogue
```

Abre: http://localhost:6006

---

### **MÃ©tricas a Comparar**

| MÃ©trica | Significado | QuÃ© Buscar |
|---------|-------------|------------|
| `rollout/ep_rew_mean` | Reward promedio | â¬†ï¸ MÃ¡s alto = mejor |
| `rollout/ep_len_mean` | Longitud de episodio | Context-dependent |
| `train/approx_kl` | Divergencia KL | Estabilidad |
| `train/explained_variance` | QuÃ© tan bien predice valor | â¬†ï¸ Cerca de 1.0 |

**HipÃ³tesis**:
- **Pure DRL**: Aprende lento pero robusto
- **Rule-based**: Aprende medio-rÃ¡pido, puede sobre-optimizar milestones
- **LLM+Dialogue**: Aprende rÃ¡pido si LLM da buenos consejos

---

## ğŸ¯ Diferencias TÃ©cnicas

### **Observation Space**

**TODOS usan el mismo observation**:
```python
observation = {
    'map': np.array([7, 7, 3]),  # Mapa 7x7 con 3 canales
    'vector': np.array([19])     # Features incluye milestone_count
}
```

Esto significa que **los modelos son compatibles** - puedes:
- Cargar un modelo pre-entrenado con cualquier modo
- Cambiar de modo durante el entrenamiento
- Comparar directamente el rendimiento

---

### **Reward Function**

**Base reward (igual para todos)**:
```python
# En drl_env.py _calculate_reward_from_lightweight()
reward = 0
reward += 1000 * badges_obtenidos
reward += 50 * level_ups
reward += 0.5 * movimiento
reward -= 0.05 * stuck_penalty
```

**Reward Shaping (diferencias)**:

| Modo | Multiplier | Fuente de DecisiÃ³n |
|------|------------|-------------------|
| Pure DRL | 1.0 (siempre) | Ninguna |
| Rule-based | 0.3 - 1.8 | Reglas if/else |
| LLM+Dialogue | 0.3 - 2.0 | LLM analiza texto |

---

## ğŸ”¬ Experimento Sugerido

### **Plan de Prueba**

1. **Entrenar los 3 modelos** con los mismos parÃ¡metros:
   ```bash
   ./train_comparison.sh
   ```

2. **Dejar entrenar** por al menos 100k timesteps cada uno

3. **Evaluar** cada modelo:
   ```bash
   python train_ppo.py --mode test \
       --model-path ./models/ppo_pure_drl \
       --test-episodes 10
   
   python train_ppo.py --mode test \
       --model-path ./models/ppo_rule_based \
       --test-episodes 10
   
   python train_ppo.py --mode test \
       --model-path ./models/ppo_llm_dialogue \
       --test-episodes 10
   ```

4. **Comparar**:
   - Badges obtenidos
   - Milestones completados
   - Tiempo de entrenamiento
   - Estabilidad

---

## ğŸ“ Logs Esperados

### **Pure DRL**
```bash
ğŸ”µ Pure DRL mode - ALL reward shaping disabled
Step 10000: reward=5.2, badges=0, milestones=3
Step 20000: reward=8.5, badges=0, milestones=5
```

### **Rule-based**
```bash
ğŸ“Š Rule-based milestone reward shaping
ğŸ¯ [Env 0] Step 10000: NEW MILESTONE! Boosting rewards. Total: 5 (multiplier=1.80x)
ğŸ’° Reward shaping: 0.50 â†’ 0.90 (LLM:1.80)
```

### **LLM+Dialogue**
```bash
ğŸ¤– LLM + Dialogue-based reward shaping
ğŸ¤– [Env 0] LLM: Dialogue says go north to find Birch | Objective: Find Professor Birch | (multiplier=1.50x)
ğŸ’° Reward shaping: 0.50 â†’ 0.75 (LLM:1.50)
```

---

## âš ï¸ Consideraciones

### **Tiempo de Entrenamiento**

| Modo | Tiempo estimado (100k steps, 4 envs) |
|------|--------------------------------------|
| Pure DRL | ~30 minutos |
| Rule-based | ~35 minutos |
| LLM+Dialogue | ~50-60 minutos (por LLM calls) |

### **Requisitos**

- **Pure DRL**: Solo Python + mGBA
- **Rule-based**: Solo Python + mGBA
- **LLM+Dialogue**: Python + mGBA + **Ollama** + Modelo LLM descargado

---

## ğŸ‰ ConclusiÃ³n

Con este sistema puedes:

âœ… **Experimentar** con diferentes enfoques de reward shaping  
âœ… **Comparar** resultados objetivamente  
âœ… **Publicar** findings cientÃ­ficos (quÃ© mÃ©todo funciona mejor)  
âœ… **Aprender** quÃ© papel juega el reward shaping en DRL  

Â¡Buena suerte con tus experimentos! ğŸš€
