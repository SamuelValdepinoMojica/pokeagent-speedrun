# ğŸ¤– Sistema LLM con Lectura de DiÃ¡logos

## ğŸ“‹ Resumen

El sistema de entrenamiento ahora incluye **lectura inteligente de diÃ¡logos** para que el LLM tome decisiones basadas en lo que el juego DICE, no solo en reglas fijas.

---

## ğŸ¯ CaracterÃ­sticas Nuevas

### **1. Milestone Count en Observation**

El agente ahora puede "ver" cuÃ¡ntos milestones ha completado:

```python
observation = {
    'map': np.array([7, 7, 3]),     # Mapa visual
    'vector': np.array([19]),        # Features
}

# vector[16] = milestone_count / 100.0  # ğŸ†• NUEVO!
```

**Beneficio**: El agente aprende que `milestone_count` subiendo = progreso.

---

### **2. Lectura de DiÃ¡logos (Memoria + OCR)**

El LLM ahora lee el texto del juego usando:

```python
dialog = memory_reader.read_dialog_with_ocr_fallback(screenshot)
```

**Prioridad**:
1. **Memoria + OCR detectan** â†’ Usa memoria (mÃ¡s preciso)
2. **Solo OCR detecta** â†’ Usa OCR (memoria fallÃ³)
3. **Solo memoria detecta** â†’ Suprime (probablemente texto residual)

---

### **3. LLM Toma Decisiones Inteligentes**

El LLM recibe:

```
Current State:
- Stationary steps: 30
- Milestones completed: 5

ğŸ“œ Game Dialogue:
"Go north to find PROFESSOR BIRCH!"
```

Y decide:

```json
{
  "multiplier": 1.5,
  "reason": "Dialogue shows objective: go north. Agent moving north.",
  "detected_objective": "Find Professor Birch to the north"
}
```

---

## ğŸ”§ ConfiguraciÃ³n

### **Requisitos**

1. **Ollama** corriendo localmente:
   ```bash
   # Instalar Ollama
   curl https://ollama.ai/install.sh | sh
   
   # Descargar modelo
   ollama pull llama3
   
   # Iniciar servidor
   ollama serve
   ```

2. **OCR** habilitado (opcional pero recomendado):
   ```python
   # Ya estÃ¡ configurado en memory_reader.py
   self._ocr_detector = create_ocr_detector()
   ```

---

### **Activar el Sistema**

En `train_ppo.py` lÃ­nea 264:

```python
llm_callback = LLMRewardCallback(
    check_frequency=1000,  # Cada 1000 steps
    use_llm=True,          # ğŸ†• ACTIVADO!
    verbose=1
)
```

---

## ğŸ“Š ComparaciÃ³n de Modos

| Modo | Usa LLM | Lee DiÃ¡logos | DecisiÃ³n | Velocidad |
|------|---------|--------------|----------|-----------|
| **Rule-Based** | âŒ | âŒ | Reglas fijas | âš¡ RÃ¡pido |
| **LLM + DiÃ¡logos** | âœ… | âœ… | Inteligente | ğŸŒ Lento |

---

## ğŸ® Ejemplos de Uso del LLM

### **Ejemplo 1: Objetivo Detectado**

```
ğŸ“œ DiÃ¡logo: "The DEVON GOODS were stolen! Go to RUSTBORO WOODS!"

ğŸ¤– LLM decide:
- Multiplier: 1.6
- Reason: "Clear objective: recover Devon Goods in Rustboro Woods"
- Detected objective: "Go to Rustboro Woods"
```

### **Ejemplo 2: Progreso Confirmado**

```
ğŸ“œ DiÃ¡logo: "You obtained the STONE BADGE!"

ğŸ¤– LLM decide:
- Multiplier: 2.0
- Reason: "Major milestone achieved: first gym badge!"
- Detected objective: "Badge obtained, look for next objective"
```

### **Ejemplo 3: Agente Atascado**

```
ğŸ“œ DiÃ¡logo: (none detected)
Stationary steps: 150

ğŸ¤– LLM decide:
- Multiplier: 0.3
- Reason: "Agent stuck with no dialogue guidance"
- Detected objective: null
```

---

## ğŸ” CÃ³mo Funciona (Interno)

### **Paso 1: Lectura de DiÃ¡logos**

```python
# En cada check (1000 steps):
screenshot = emulator.get_screenshot()
dialog = memory_reader.read_dialog_with_ocr_fallback(screenshot)
```

### **Paso 2: AnÃ¡lisis del LLM**

```python
prompt = f"""
Current State:
- Milestones: {completed}
- Stationary: {stationary_steps}

ğŸ“œ Game Dialogue:
"{dialog_text}"

Analyze: What is the objective? Is agent progressing?
"""

response = ollama.generate(prompt)
```

### **Paso 3: Aplicar Multiplicador**

```python
multiplier = llm_output['multiplier']  # 0.3 - 2.0
reward_final = reward_base * multiplier

# Ejemplo:
0.50 Ã— 1.8 = 0.90  # Boost por milestone
```

---

## âš™ï¸ Ventajas vs Desventajas

### âœ… **Ventajas**

1. **Objetivos del Juego**: El LLM lee lo que el juego DICE hacer
2. **Adaptativo**: Aprende patrones de diÃ¡logo â†’ objetivo
3. **Legal**: Solo usa informaciÃ³n que el jugador ve
4. **Milestone Awareness**: El agente sabe cuÃ¡nto progreso lleva

### âŒ **Desventajas**

1. **Lento**: Llamada a LLM cada 1000 steps (~3-5 segundos)
2. **Requiere Ollama**: Debe estar corriendo en localhost
3. **OCR Imperfecto**: Puede malinterpretar texto
4. **Cambio de Observation**: Requiere reentrenar modelo (vector[19] vs vector[18])

---

## ğŸš€ PrÃ³ximos Pasos

### **Para Probar**:

```bash
# 1. Iniciar Ollama
ollama serve

# 2. Entrenar con LLM + DiÃ¡logos
python train_ppo.py \
    --mode train \
    --state Emerald-GBAdvance/quick_start_save.state \
    --timesteps 100000 \
    --n-envs 4

# 3. Monitorear logs
tail -f training.log | grep "ğŸ¤–"
```

### **Logs Esperados**:

```
ğŸ¤– [Env 0] LLM: Dialogue shows objective: go to Route 101 | Objective: Find Professor Birch | (multiplier=1.50x)
ğŸ’° Reward shaping: 0.50 â†’ 0.75 (LLM:1.50)
```

---

## ğŸ“ˆ Resultados Esperados

**HipÃ³tesis**: 
- El agente aprenderÃ¡ **mÃ¡s rÃ¡pido** porque el LLM interpreta objetivos del juego
- Menos tiempo atascado en lugares sin objetivo
- Mejor alineaciÃ³n con progreso real del juego

**MÃ©tricas a Monitorear**:
- Milestones por episodio
- Tiempo hasta primer badge
- Reward promedio
- Objetivos detectados por el LLM

---

## ğŸ”„ Volver a Rule-Based

Si el LLM es muy lento o no funciona bien:

```python
# En train_ppo.py lÃ­nea 266:
use_llm=False  # Desactivar LLM
```

VolverÃ¡ al sistema de reglas fijas (mÃ¡s rÃ¡pido).

---

## ğŸ“ Notas TÃ©cnicas

### **Cambios en Observation Space**

```python
# ANTES:
'vector': spaces.Box(shape=(18,))

# AHORA:
'vector': spaces.Box(shape=(19,))  # +1 para milestone_count
```

**ImplicaciÃ³n**: Modelos pre-entrenados con vector[18] **NO funcionarÃ¡n**. Hay que reentrenar desde cero.

### **Acceso a Milestone Tracker**

```python
# En lightweight_state_reader.py:
if hasattr(self.mem, 'core') and hasattr(self.mem.core, 'milestone_tracker'):
    milestone_count = len(milestone_tracker.milestones)
```

---

## âœ¨ ConclusiÃ³n

Este sistema representa un **hÃ­brido inteligente**:

- **PPO** aprende polÃ­tica (quÃ© botones presionar)
- **Milestone Count** en observation (sabe si progresa)
- **LLM + DiÃ¡logos** para reward shaping (interpreta objetivos)

Es como tener un **entrenador humano** que lee el juego y dice "bien hecho" o "estÃ¡s atascado".

Â¡Buena suerte con el entrenamiento! ğŸ®ğŸ¤–
