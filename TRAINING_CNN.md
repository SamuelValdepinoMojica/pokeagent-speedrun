# ğŸ® Pokemon Emerald DRL con CNN

Entrenamiento de agentes de Deep Reinforcement Learning (PPO) para Pokemon Emerald usando CNNs para procesar el mapa.

## ğŸ—ï¸ Arquitectura

### **ObservaciÃ³n (Observation Space)**

El agente recibe dos tipos de informaciÃ³n:

1. **Mapa (7x7x3)** - Procesado con CNN
   - Canal 0: **Metatile ID** (tipo de tile: grass, water, door, etc.)
   - Canal 1: **Behavior** (comportamiento: walkable, surf, encounter, etc.)
   - Canal 2: **Collision** (0 = puede caminar, 1 = bloqueado)

2. **Vector (18 features)** - Procesado con MLP
   - PosiciÃ³n del jugador (x, y)
   - Party PokÃ©mon (6 x 2 = 12): nivel y HP% de cada PokÃ©mon
   - Estado del juego (4): dinero, badges, en batalla, pokÃ©dex

### **PolÃ­tica CNN Personalizada**

```
Map (7x7x3) â”€â”€â–º CNN â”€â”€â”€â”€â”€â”€â”
                          â”œâ”€â”€â–º Fusion â”€â”€â–º Actor/Critic
Vector (18) â”€â”€â–º MLP â”€â”€â”€â”€â”€â”€â”˜

CNN: Conv2D(3â†’32) â†’ Conv2D(32â†’64) â†’ Flatten
MLP: Linear(18â†’64) â†’ Linear(64â†’128)
Fusion: Concat â†’ Linear(combinedâ†’256)
```

## ğŸ“¦ InstalaciÃ³n

```bash
# 1. Instalar dependencias bÃ¡sicas
pip install gymnasium stable-baselines3[extra] tensorboard

# 2. Instalar dependencias del proyecto
pip install mgba-py pillow numpy

# 3. (Opcional) Para GPU
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

## ğŸš€ Uso RÃ¡pido

### **1. Probar el entorno**

```bash
python test_cnn_env.py
```

Esto verificarÃ¡:
- âœ… El entorno se inicializa correctamente
- âœ… Las observaciones tienen la forma correcta
- âœ… Los canales del mapa contienen datos vÃ¡lidos
- âœ… Las acciones funcionan

### **2. Entrenar el agente**

```bash
# Entrenamiento bÃ¡sico (1M steps, ~1-2 horas en GPU)
python train_ppo.py --mode train --timesteps 1000000

# Entrenamiento largo (10M steps, ~10-20 horas en GPU)
python train_ppo.py --mode train --timesteps 10000000

# Con estado personalizado
python train_ppo.py --mode train \
    --state Emerald-GBAdvance/truck_start.state \
    --timesteps 5000000
```

### **3. Monitorear entrenamiento**

En otra terminal:

```bash
tensorboard --logdir ./tensorboard_logs
```

Abre http://localhost:6006 para ver:
- Reward por episodio
- PÃ©rdidas del actor/critic
- Badges obtenidas
- Locaciones visitadas

### **4. Probar modelo entrenado**

```bash
python train_ppo.py --mode test \
    --model-path ./models/ppo_pokemon.zip \
    --test-episodes 10
```

## ğŸ¯ Recompensas (Reward Shaping)

El agente aprende con estas recompensas:

| Evento | Recompensa | DescripciÃ³n |
|--------|------------|-------------|
| ğŸ† Badge obtenida | +1000.0 | Objetivo principal |
| ğŸ“ˆ Subir de nivel | +50.0 | Por cada nivel ganado (suma de party) |
| ğŸ—ºï¸ Nueva ubicaciÃ³n | +20.0 | Primera vez en ubicaciÃ³n |
| ğŸš¶ Regresar a ubicaciÃ³n | +5.0 | UbicaciÃ³n ya visitada |
| ğŸƒ Moverse | +0.5 | Por cada paso dado |
| ğŸ§ Quedarse quieto | -0.1 a -1.0 | PenalizaciÃ³n creciente |
| ğŸ’” HP crÃ­tico (<20%) | -5.0 | Por PokÃ©mon con HP bajo |
| ğŸ©¹ HP bajo (20-50%) | -1.0 | Por PokÃ©mon con HP medio |

## ğŸ“Š Arquitectura CNN Detallada

### **Feature Extractor**

```python
PokemonCNNExtractor(
    # CNN para mapa
    map_cnn: Sequential(
        Conv2d(3, 32, kernel_size=3, padding=1),  # 7x7x3 â†’ 7x7x32
        ReLU(),
        BatchNorm2d(32),
        Conv2d(32, 64, kernel_size=3, padding=1), # 7x7x32 â†’ 7x7x64
        ReLU(),
        BatchNorm2d(64),
        Flatten()  # 7x7x64 = 3136 features
    ),
    
    # MLP para vector
    vector_mlp: Sequential(
        Linear(18, 64),
        ReLU(),
        Linear(64, 128),
        ReLU()
    ),
    
    # Fusion
    fusion: Sequential(
        Linear(3136+128=3264, 256),  # Combina ambas ramas
        ReLU()
    )
)
```

### **Actor-Critic**

```python
Policy Network:
  features (256) â†’ actor_net â†’ action_logits (8)
  features (256) â†’ critic_net â†’ value (1)
```

## ğŸ”§ HiperparÃ¡metros

```python
PPO(
    learning_rate=3e-4,      # Tasa de aprendizaje
    n_steps=2048,            # Steps por actualizaciÃ³n
    batch_size=64,           # TamaÃ±o de batch
    n_epochs=10,             # Ã‰pocas por actualizaciÃ³n
    gamma=0.99,              # Factor de descuento
    gae_lambda=0.95,         # GAE lambda
    clip_range=0.2,          # PPO clip range
    ent_coef=0.01,           # Coeficiente de entropÃ­a
)
```

## ğŸ“ Estructura de Archivos

```
agent/
  â”œâ”€â”€ drl_env.py           # Entorno Gymnasium con CNN observations
  â”œâ”€â”€ cnn_policy.py        # PolÃ­tica CNN personalizada
  â””â”€â”€ __init__.py

train_ppo.py               # Script de entrenamiento
test_cnn_env.py            # Script de prueba

models/                    # Modelos entrenados guardados aquÃ­
logs/                      # Logs de entrenamiento
  â””â”€â”€ checkpoints/         # Checkpoints cada 10k steps
tensorboard_logs/          # Logs para TensorBoard
```

## ğŸ® Por quÃ© usar CNN para el mapa?

### **Ventajas:**

1. **Patrones espaciales**: La CNN aprende a reconocer:
   - Caminos vs obstÃ¡culos
   - Puertas y entradas
   - Agua para Surf
   - Hierba alta (encuentros)

2. **Invarianza a traslaciÃ³n**: Reconoce un camino sin importar dÃ³nde estÃ© en el grid

3. **Features jerÃ¡rquicas**: 
   - Primeras capas: bordes, tiles individuales
   - Capas profundas: patrones complejos (habitaciones, corredores)

4. **Mejor que vectorizar**: Un vector plano pierde informaciÃ³n espacial

### **Ejemplo visual:**

```
Mapa crudo:          CNN ve:
â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘           [Camino vertical]
â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘           [Paredes a los lados]
â–ˆâ–ˆâ–ˆâ–ˆâ–“â–“â–“â–ˆâ–ˆâ–ˆâ–ˆ  â”€â”€â”€â–º   [Puerta en el centro]
â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘           [Camino continÃºa]
â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘â–‘           [Navegable hacia arriba]
```

## ğŸš¦ PrÃ³ximos Pasos

### **Mejoras posibles:**

1. **Curriculum Learning**: Entrenar progresivamente en objetivos mÃ¡s difÃ­ciles
2. **Reward Shaping con LLM**: Usar LLM local para objetivos dinÃ¡micos
3. **Multi-Objetivo**: Entrenar para mÃºltiples badges simultÃ¡neamente
4. **Attention Mechanism**: Agregar attention sobre el mapa
5. **Recurrencia**: Usar LSTM/GRU para memoria temporal

### **Experimentar con arquitectura:**

```python
# En cnn_policy.py, puedes modificar:

# MÃ¡s capas convolucionales
Conv2d(64, 128, kernel_size=3),
Conv2d(128, 256, kernel_size=3),

# Pooling para reducir dimensionalidad
MaxPool2d(2, 2),

# Residual connections
x = x + conv_block(x)
```

## ğŸ› Troubleshooting

**Error: "No module named 'mgba'"**
```bash
pip install mgba-py
```

**Error: CUDA out of memory**
```python
# Reducir batch_size en train_ppo.py
batch_size=32  # En lugar de 64
```

**Entrenamiento muy lento**
```bash
# Verificar que usa GPU
python -c "import torch; print(torch.cuda.is_available())"

# Si no hay GPU, reducir n_steps
n_steps=512  # En lugar de 2048
```

**Agente se queda atascado**
```python
# Aumentar penalizaciÃ³n por quedarse quieto
reward -= 0.5 * min(self.stationary_steps, 10)  # En _calculate_reward
```

## ğŸ“ Notas

- El entorno usa `load_state()` para reset, no reinicia el juego completo
- Los checkpoints se guardan cada 10,000 steps por defecto
- El entrenamiento es determinista si usas `seed` en `reset()`
- La CNN procesa tiles en formato (H, W, C), PyTorch usa (C, H, W)

## ğŸ¤ Contribuir

Mejoras sugeridas:
- [ ] Agregar mÃ¡s canales al mapa (NPC positions, items)
- [ ] Implementar HER (Hindsight Experience Replay)
- [ ] Multi-agent training con self-play
- [ ] VisualizaciÃ³n en tiempo real del training

---

Â¡Buena suerte entrenando! ğŸš€ğŸ®
