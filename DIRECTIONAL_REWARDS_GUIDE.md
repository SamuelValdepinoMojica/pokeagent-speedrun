# ğŸ§­ GuÃ­a de Recompensas Direccionales

## Â¿QuÃ© es Reward Shaping Direccional?

Es un sistema que **recompensa progresivamente** al agente cuando se acerca a objetivos importantes, **antes** de completarlos.

---

## ğŸ¯ Problema Anterior vs SoluciÃ³n Actual

### âŒ Antes (Solo Milestones)

```
Agente en ROUTE 101:
â”œâ”€ Objetivo: Rescatar a Prof Birch (en posiciÃ³n 13, 7)
â”œâ”€ PosiciÃ³n actual: (5, 5) â†’ distancia = 15 tiles
â”‚
â”œâ”€ Agente camina hacia objetivo:
â”‚  â””â”€ (6, 5) â†’ (7, 5) â†’ (8, 6) â†’ (9, 7)...
â”‚     â””â”€ Reward: +0.5 cada paso (movimiento)
â”‚     â””â”€ Multiplier: 1.0Ã— (normal, sin guÃ­a)
â”‚
â””â”€ Finalmente llega al objetivo:
   â””â”€ Â¡Rescata a Birch! â†’ +1000 reward
   â””â”€ Milestone completado â†’ Multiplier: 1.8Ã—

PROBLEMA: Solo recompensa al FINAL, no durante el camino
```

### âœ… Ahora (Direccional + Milestones)

```
Agente en ROUTE 101:
â”œâ”€ Objetivo detectado: Prof Birch en (13, 7)
â”œâ”€ PosiciÃ³n inicial: (5, 5) â†’ distancia = 15 tiles
â”‚
â”œâ”€ Agente se mueve HACIA el objetivo:
â”‚  â”œâ”€ (6, 5) â†’ distancia = 14 tiles
â”‚  â”‚  â””â”€ ğŸ§­ "AcercÃ¡ndose" â†’ Multiplier: 1.5Ã—
â”‚  â”‚  â””â”€ Reward: 0.5 â†’ 0.75 (boosted!)
â”‚  â”‚
â”‚  â”œâ”€ (7, 5) â†’ distancia = 13 tiles
â”‚  â”‚  â””â”€ ğŸ§­ "AcercÃ¡ndose" â†’ Multiplier: 1.5Ã—
â”‚  â”‚  â””â”€ Reward: 0.5 â†’ 0.75 (boosted!)
â”‚  â”‚
â”‚  â””â”€ (12, 7) â†’ distancia = 1 tile
â”‚     â””â”€ ğŸ¯ "Â¡MUY CERCA!" â†’ Multiplier: 1.8Ã—
â”‚     â””â”€ Reward: 0.5 â†’ 0.90 (gran boost!)
â”‚
â”œâ”€ Agente se mueve LEJOS del objetivo:
â”‚  â””â”€ (6, 4) â†’ distancia = 16 tiles (+1)
â”‚     â””â”€ âš ï¸ "AlejÃ¡ndose" â†’ Multiplier: 0.8Ã—
â”‚     â””â”€ Reward: 0.5 â†’ 0.40 (penalizado)
â”‚
â””â”€ Finalmente rescata a Birch:
   â””â”€ Milestone â†’ Multiplier LLM: 1.8Ã—
   â””â”€ Muy cerca â†’ Multiplier Dir: 1.8Ã—
   â””â”€ COMBINADO: 1.8 Ã— 1.8 = 3.24Ã— Â¡Super boost!

SOLUCIÃ“N: Recompensa CONTINUAMENTE el progreso correcto
```

---

## ğŸ“Š Ejemplo Real de Training

### Sin Direccional (solo LLM):
```bash
Step 1000: 
ğŸ” LLM Callback: NEW MILESTONE! (Ã—1.80)
ğŸ’° Reward: 0.50 â†’ 0.90 (Ã—1.80)  # Solo cuando completa milestone

Steps 1001-1999:
ğŸ’° Reward: 0.50 â†’ 0.50 (Ã—1.00)  # Normal, sin guÃ­a
ğŸ’° Reward: 0.50 â†’ 0.50 (Ã—1.00)
ğŸ’° Reward: 0.50 â†’ 0.50 (Ã—1.00)
...
# Agente camina sin saber si va bien o mal
```

### Con Direccional (LLM + Proximidad):
```bash
Step 100:
ğŸ§­ [Env 0] âœ… AcercÃ¡ndose a objetivo en ROUTE_101 (-2.0 tiles) (Ã—1.50)
ğŸ’° Reward: 0.50 â†’ 0.75 (Dir:1.50)

Step 200:
ğŸ§­ [Env 0] âœ… AcercÃ¡ndose a objetivo en ROUTE_101 (-1.5 tiles) (Ã—1.50)
ğŸ’° Reward: 0.50 â†’ 0.75 (Dir:1.50)

Step 300:
ğŸ§­ [Env 0] ğŸ¯ Â¡MUY CERCA del objetivo! (2.0 tiles) (Ã—1.80)
ğŸ’° Reward: 0.50 â†’ 0.90 (Dir:1.80)

Step 400:
ğŸ§­ [Env 0] âš ï¸ AlejÃ¡ndose de objetivo en ROUTE_101 (+1.0 tiles) (Ã—0.80)
ğŸ’° Reward: 0.50 â†’ 0.40 (Dir:0.80)

Step 500:
ğŸ§­ [Env 0] âœ… AcercÃ¡ndose a objetivo en ROUTE_101 (-1.0 tiles) (Ã—1.50)
ğŸ’° Reward: 0.50 â†’ 0.75 (Dir:1.50)

Step 1000:
ğŸ” LLM Callback: NEW MILESTONE! (Ã—1.80)
ğŸ§­ [Env 0] ğŸ¯ Â¡MUY CERCA del objetivo! (0.5 tiles) (Ã—1.80)
ğŸ’° Reward: 0.50 â†’ 1.62 (LLM:1.80 Ã— Dir:1.80)  # Â¡3.24Ã— combinado!
```

---

## ğŸ—ºï¸ Objetivos Configurados

El sistema conoce objetivos importantes en cada mapa:

```python
ROUTE_101: 
  â””â”€ (13, 7) "Prof Birch rescue"

LITTLEROOT_TOWN:
  â”œâ”€ (14, 8) "Player's house"
  â””â”€ (7, 8) "Rival's house"

ROUTE_103:
  â””â”€ (4, 4) "Rival battle"

OLDALE_TOWN:
  â”œâ”€ (8, 8) "Pokemon Center"
  â””â”€ (13, 7) "Mart"

PETALBURG_CITY:
  â””â”€ (13, 13) "Gym entrance"
```

**Â¿CÃ³mo funciona?**
- Cada 100 steps, el sistema calcula la distancia al objetivo mÃ¡s cercano
- Si la distancia disminuye â†’ **Boost** (1.5Ã—)
- Si la distancia aumenta â†’ **PenalizaciÃ³n** (0.8Ã—)
- Si estÃ¡ muy cerca (<3 tiles) â†’ **Super Boost** (1.8Ã—)

---

## ğŸ® CÃ³mo Usar

### Entrenar con Direccional:
```bash
python train_ppo.py --mode train --timesteps 100000 --n-envs 4
```

El sistema ahora tiene **2 capas de reward shaping**:

1. **LLM Callback** (cada 1000 steps):
   - Detecta milestones completados
   - Detecta si estÃ¡ atascado
   - Multiplier: 0.3Ã— a 1.8Ã—

2. **Directional Callback** (cada 100 steps):
   - Detecta si se acerca a objetivos
   - GuÃ­a continuamente al agente
   - Multiplier: 0.8Ã— a 1.8Ã—

**Multiplicadores se COMBINAN:**
```
Final reward = base_reward Ã— llm_multiplier Ã— directional_multiplier

Ejemplo:
- Base: 0.5 (movimiento)
- LLM: 1.8Ã— (milestone)
- Direccional: 1.5Ã— (acercÃ¡ndose)
- FINAL: 0.5 Ã— 1.8 Ã— 1.5 = 1.35 ğŸš€
```

---

## ğŸ”§ Agregar Nuevos Objetivos

Edita `agent/directional_reward_callback.py`:

```python
self.known_objectives = {
    "NOMBRE_MAPA": [(x, y, "descripcion")],
    
    # Ejemplo: Agregar Rustboro City
    "RUSTBORO_CITY": [
        (15, 20, "Gym entrance"),
        (10, 15, "Devon Corp"),
        (8, 8, "Pokemon Center")
    ],
}
```

---

## ğŸ“ˆ Beneficios

### Aprendizaje mÃ¡s Eficiente:
- âœ… Agente recibe feedback **cada 100 steps** (antes: solo cuando completa milestone)
- âœ… Aprende **direcciÃ³n correcta** mÃ¡s rÃ¡pido
- âœ… Menos tiempo "perdido" caminando sin propÃ³sito

### Menos Tiempo Atascado:
- âœ… Si se aleja del objetivo â†’ penalizaciÃ³n inmediata
- âœ… Fuerza al agente a **probar nuevas direcciones**

### Mejor Progreso en Historia:
- âœ… Objetivos alineados con progreso del juego
- âœ… Sistema guÃ­a hacia milestones naturalmente
- âœ… CombinaciÃ³n con LLM multiplica efectividad

---

## ğŸ§ª Testing

Ver los logs en tiempo real:
```bash
# Filtrar solo reward shaping
tail -f training.log | grep -E "ğŸ§­|ğŸ’°|ğŸ”"
```

Ejemplo de output esperado:
```
ğŸ§­ [Env 0] âœ… AcercÃ¡ndose a objetivo en ROUTE_101 (Ã—1.50)
ğŸ’° Reward: 0.50 â†’ 0.75 (Dir:1.50)
ğŸ§­ [Env 1] ğŸ¯ Â¡MUY CERCA del objetivo! (Ã—1.80)
ğŸ’° Reward: 0.50 â†’ 0.90 (Dir:1.80)
ğŸ” LLM Callback: NEW MILESTONE! Total: 8 (Ã—1.80)
ğŸ’° Reward: 0.50 â†’ 1.62 (LLM:1.80 Ã— Dir:1.80)
```

---

## âš™ï¸ ConfiguraciÃ³n Avanzada

Ajustar multiplicadores en `train_ppo.py`:

```python
directional_callback = DirectionalRewardCallback(
    check_frequency=100,      # Revisar cada N steps
    proximity_boost=1.5,      # Multiplicador cuando se acerca
    proximity_penalty=0.8,    # Multiplicador cuando se aleja
    verbose=1
)
```

**Recomendaciones:**
- `check_frequency=100`: Balance entre precisiÃ³n y performance
- `proximity_boost=1.5`: Suficiente incentivo sin dominar otros rewards
- `proximity_penalty=0.8`: PenalizaciÃ³n suave (no destruye el learning)

---

## ğŸ¯ ConclusiÃ³n

El sistema ahora responde a tu pregunta original:

> "Â¿No serÃ­a mejor que sepa si estÃ¡ en el lugar correcto?"

**Respuesta: Â¡SÃ!** Y ahora lo sabe. El sistema:

1. âœ… Detecta objetivos importantes en cada mapa
2. âœ… Calcula distancia al objetivo mÃ¡s cercano
3. âœ… Recompensa continuamente cuando se acerca
4. âœ… Penaliza cuando se aleja sin razÃ³n
5. âœ… Combina con LLM para mÃ¡xima efectividad

Â¡Todo sin necesidad de visiÃ³n compleja o LLM costoso!
